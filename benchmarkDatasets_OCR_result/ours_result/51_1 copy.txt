tion over Z does not significantly impact the approach described #0 far, but the use of
latent variables is s0 ubiquitous that it deserves special treatment.

In particular, some insight can be gained by viewing the inference process in the
presence of latent variables as a simultaneous minimization over ¥ and Z:

¥* = argminy¢,ze22(Z,Y,X) «a

Latent variables can be viewed as intermediate results on the way to finding the best
‘oulput Y.. At this point, one could argue that there is no conceptual difference between
the Z and Y variables: Z could simply be folded into ¥.. The distinction arises during
training: we are given the correct value of Y’ for a number of training samples, but we

are never given the correct value of Z.
Latent variables are very useful in situations where a hidden characteristic of the

process being modeled can be inferred from observations, but cannot be predicted di
rectly. One such example is in recognition problems, For example, in face recognition
the gender of a person or the orientation of the face could be a latent variable. Knowing
these values would make the recognition task much easier. Likewise in invariant object
recognition the pose parameters of the object (location, orientation, scale) or the illumi

nation could be latent variables. They play a crucial role in problems where segmenta-
tion of the sequential data must be performed simultaneously with the recognition task

‘A good example is speech recognition, in which the segmentation of sentences into
‘words and words into phonemes must take place simultaneously with recognition, yet
the correct segmentation into phonemes is rarely available during training. Similarly, in
handwriting recognition, the segmentation of words into characters should take place
simultaneously with the recognition. The use of latent variables in face recognition is
discussed in this section, and Section 7.3 describes a latent variable architecture for
handwriting recognition.

4.1 An Example of Latent Variable Architecture

To illustrate the concept of latent variables, we consider the task of face detection,
beginning with the simple problem of determining whether a face is present or not in
a small image, Imagine that we are provided with a face detecting function Gface(X)
which takes a small image window as input and produces a scalar output, It outputs
‘4 small value when a human face fills the input image, and a large value if no face is
present (or if only a piece of a face or a tiny face is present). An energy-based face
‘detector buill around this function is shown in Figure 8(a). The variable ¥ controls the
position of a binary switch (1 = “face”, 0 = “non-face”). The output energy is equal
10 Graco(X) when Y = 1, and to a fixed threshold value 7’ when Y = 0:
E(Y,X) = ¥Gtnee(X) + (1 = Y)T-

‘The value of ¥ that minimizes this
pias yen mg energy function is 1 (face) if Gigee(X) < T and 0

Let us now consider the more complex task of detecting and locating a single face

ina large image. We can apply our Grace(X) function to multiple windows in the large
image, compute which window produces the lowest value of Giace(X), and detect a

2
